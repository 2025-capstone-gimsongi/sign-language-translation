# -*- coding: utf-8 -*-
import cv2
import mediapipe as mp
import numpy as np
from keras.models import load_model
import joblib
from collections import deque
from PIL import ImageFont, ImageDraw, Image

# ==========================
# ì„¤ì •
# ==========================
MODEL_PATH = "/Users/kyungrim/Library/CloudStorage/GoogleDrive-20221999@edu.hanbat.ac.kr/ë‚´ ë“œë¼ì´ë¸Œ/2025ìº¡ìŠ¤í†¤í”„ë¡œì íŠ¸/ASL_lstm/models/gesture_lstm_model_dual_small_v1.h5"
ENCODER_PATH = "/Users/kyungrim/Library/CloudStorage/GoogleDrive-20221999@edu.hanbat.ac.kr/ë‚´ ë“œë¼ì´ë¸Œ/2025ìº¡ìŠ¤í†¤í”„ë¡œì íŠ¸/ASL_lstm/processed_lstm/label_encoder_lstm_dual.pkl"
FONT_PATH = "/System/Library/Fonts/AppleSDGothicNeo.ttc"

FRAMES_PER_SEQUENCE = 30        # í•™ìŠµ ë•Œë„ 30í”„ë ˆì„ ì‚¬ìš©
CONFIDENCE_THRESHOLD = 0.75     # í™”ë©´ì— ì´ˆë¡ìƒ‰ìœ¼ë¡œ í‘œì‹œí•  ê¸°ì¤€
PREDICTION_INTERVAL = 3         # ëª‡ í”„ë ˆì„ë§ˆë‹¤ í•œ ë²ˆì”©ë§Œ ì˜ˆì¸¡í• ì§€
IDLE_CLEAR_FRAMES = 5           # ì†ì´ ì•ˆ ë³´ì´ëŠ” í”„ë ˆì„ì´ ì´ë§Œí¼ ìŒ“ì´ë©´ ë²„í¼/ì˜ˆì¸¡ ë¦¬ì…‹

# ==========================
# í•œê¸€ ì¶œë ¥ í•¨ìˆ˜
# ==========================
_font_cache = {}

def draw_korean_text(img, text, position, font_size=32, color=(255, 255, 255), max_width=None):
    if font_size not in _font_cache:
        _font_cache[font_size] = ImageFont.truetype(FONT_PATH, font_size)

    font = _font_cache[font_size]

    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)

    x, y = position
    line_height = font_size + 5
    if max_width is None:
        max_width = img.shape[1] - x - 20

    words = text.split()
    current_line = []
    current_y = y

    for w in words:
        test_line = ' '.join(current_line + [w])
        bbox = font.getbbox(test_line)
        text_width = bbox[2] - bbox[0]

        if text_width < max_width:
            current_line.append(w)
        else:
            draw.text((x, current_y), ' '.join(current_line), font=font, fill=color)
            current_y += line_height
            current_line = [w]

    if current_line:
        draw.text((x, current_y), ' '.join(current_line), font=font, fill=color)

    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

# ===============================================
# ğŸ”¥ í›ˆë ¨ ë°ì´í„°ì™€ ë™ì¼í•œ ë°©ì‹ì˜ landmark â†’ feature í•¨ìˆ˜
# ===============================================
def extract_one_frame(results):
    """
    Mediapipe ê²°ê³¼ì—ì„œ í•œ í”„ë ˆì„(30 x 126 ì¤‘ 126 ë¶€ë¶„)ì„ ë§Œë“œëŠ” í•¨ìˆ˜.
    - ì™¼ì†/ì˜¤ë¥¸ì† ê°ê° 21ê°œ ëœë“œë§ˆí¬ x (x,y,z) = 63
    - ì™¼ì† 63 + ì˜¤ë¥¸ì† 63 = 126
    - ì†ëª© ê¸°ì¤€ ìƒëŒ€ì¢Œí‘œë¡œ ì •ê·œí™” (train ì½”ë“œì™€ ë™ì¼)
    """
    hand_data = {"Left": None, "Right": None}
    hand_detected = {"Left": False, "Right": False}

    if results.multi_handedness and results.multi_hand_landmarks:
        for lm_list, handed in zip(results.multi_hand_landmarks, results.multi_handedness):
            label = handed.classification[0].label  # "Left" or "Right"

            coords = []
            for lm in lm_list.landmark:
                coords.extend([lm.x, lm.y, lm.z])
            hand_data[label] = coords
            hand_detected[label] = True

    # --- ì™¼ì† ì •ê·œí™” ---
    left_norm = [0.0] * 63
    if hand_detected["Left"]:
        left_np = np.array(hand_data["Left"]).reshape(21, 3)
        wrist = left_np[0]
        rel = left_np - wrist
        left_norm = rel.flatten().tolist()

    # --- ì˜¤ë¥¸ì† ì •ê·œí™” ---
    right_norm = [0.0] * 63
    if hand_detected["Right"]:
        right_np = np.array(hand_data["Right"]).reshape(21, 3)
        wrist = right_np[0]
        rel = right_np - wrist
        right_norm = rel.flatten().tolist()

    return left_norm + right_norm

# ==========================
# ëª¨ë¸ ë¡œë“œ
# ==========================
print("ğŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘...")
model = load_model(MODEL_PATH)
label_encoder = joblib.load(ENCODER_PATH)
print("âœ… ëª¨ë¸ ë° LabelEncoder ë¡œë“œ ì™„ë£Œ!")

# ==========================
# Mediapipe ì´ˆê¸°í™”
# ==========================
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=2,
    min_detection_confidence=0.5
)
mp_drawing = mp.solutions.drawing_utils

# ==========================
# 30í”„ë ˆì„ ë²„í¼
# ==========================
frame_buffer = deque(maxlen=FRAMES_PER_SEQUENCE)

# ==========================
# ì¹´ë©”ë¼ ì‹œì‘
# ==========================
cap = cv2.VideoCapture(1)   # í•„ìš”í•˜ë©´ 0ìœ¼ë¡œ ë°”ê¿”ì„œ ë‹¤ë¥¸ ì¹´ë©”ë¼ í…ŒìŠ¤íŠ¸
if not cap.isOpened():
    print("âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

print("â–¶ ì‹¤ì‹œê°„ LSTM í…ŒìŠ¤íŠ¸ ì‹œì‘ ('q' ì¢…ë£Œ)")

prediction_result = ("", 0.0)
is_predicting = False
frame_count = 0
no_hand_frames = 0   # ì†ì´ ì•ˆ ì¡íŒ í”„ë ˆì„ ì¹´ìš´í„°

# ==========================
# ë©”ì¸ ë£¨í”„
# ==========================
while True:
    ret, frame = cap.read()
    if not ret:
        break

    frame = cv2.flip(frame, 1)
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = hands.process(rgb)

    # === ì† ì¸ì‹ ì—¬ë¶€ì— ë”°ë¼ ì²˜ë¦¬ ===
    if results.multi_hand_landmarks:
        # í™”ë©´ì— ëœë“œë§ˆí¬ ê·¸ë ¤ì£¼ê¸° (ë””ë²„ê¹…ìš©)
        for hand_landmarks in results.multi_hand_landmarks:
            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

        # ì†ì´ ë³´ì´ë©´ ë²„í¼ì— í”„ë ˆì„ ì¶”ê°€
        one_frame = extract_one_frame(results)
        frame_buffer.append(one_frame)

        # ì† ë³´ì´ê¸° ì‹œì‘í–ˆìœ¼ë‹ˆ idle ì¹´ìš´í„° ë¦¬ì…‹
        no_hand_frames = 0
    else:
        # ì†ì´ ì•ˆ ë³´ì´ëŠ” í”„ë ˆì„ ëˆ„ì 
        no_hand_frames += 1

        # ì¼ì • í”„ë ˆì„ ì´ìƒ ì†ì´ ì•ˆ ë³´ì´ë©´ ì™„ì „ idle ì²˜ë¦¬
        if no_hand_frames >= IDLE_CLEAR_FRAMES:
            frame_buffer.clear()
            prediction_result = ("", 0.0)  # í™”ë©´ì—ì„œ ì˜ˆì¸¡ í…ìŠ¤íŠ¸ ì œê±°
            # ì—¬ê¸°ì„œ ë°”ë¡œ no_hand_frames ê³„ì† ì˜¬ë ¤ë„ ë˜ì§€ë§Œ,
            # ì´ë¯¸ idle ìƒíƒœë¼ ì‚¬ì‹¤ í° ì˜ë¯¸ëŠ” ì—†ìŒ

    frame_count += 1

    # === ì˜ˆì¸¡ (ì†ì´ ë³´ì´ê³ , ë²„í¼ê°€ ê½‰ ì°¼ê³ , ì¼ì • ì£¼ê¸°ë§ˆë‹¤ë§Œ) ===
    if (
        len(frame_buffer) == FRAMES_PER_SEQUENCE
        and not is_predicting
        and no_hand_frames == 0                    # ë°”ë¡œ ì§ì „ì—ë„ ì†ì´ ë³´ì˜€ì„ ë•Œë§Œ
        and frame_count % PREDICTION_INTERVAL == 0 # ì˜ˆì¸¡ ì£¼ê¸° ì œì–´
    ):
        is_predicting = True

        seq = np.array(frame_buffer).reshape(1, FRAMES_PER_SEQUENCE, 126).astype("float32")
        pred = model.predict(seq, verbose=0)
        conf = float(np.max(pred))
        idx = int(np.argmax(pred))
        label = label_encoder.inverse_transform([idx])[0]

        prediction_result = (label, conf)
        is_predicting = False

    # === í™”ë©´ í‘œì‹œ ===
    label, conf = prediction_result
    if label:
        text = f"Predict: {label} ({conf:.2f})"
    else:
        text = "Predict: (none)"

    color = (0, 255, 0) if conf >= CONFIDENCE_THRESHOLD else (255, 0, 0)
    frame = draw_korean_text(frame, text, (10, 30), font_size=40, color=color)

    cv2.imshow("LSTM Sign Test", frame)
    key = cv2.waitKey(1) & 0xFF

    if key == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()