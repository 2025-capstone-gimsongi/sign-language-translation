# ìˆ˜ì–´ ì¸ì‹ìš© ì¹´ë©”ë¼ + ì¸ì‹ ëª¨ë¸ ì‚¬ìš© ì½”ë“œ

import cv2
import mediapipe as mp
import numpy as np
from keras.models import load_model
import joblib
from collections import deque
from PIL import ImageFont, ImageDraw, Image

# --- ì„¤ì •ê°’ ---
# ğŸŒŸ v2 ëª¨ë¸ ê²½ë¡œë¡œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
MODEL_PATH = "models/gesture_lstm_model_dual_v2.h5" 
ENCODER_PATH = "processed_lstm/label_encoder_lstm_dual.pkl"
# ğŸŒŸ í•™ìŠµ ì‹œ ì‚¬ìš©í•œ í”„ë ˆì„ ìˆ˜ì™€ ë™ì¼í•˜ê²Œ ë§ì¶°ì£¼ì„¸ìš”.
FRAMES_PER_SEQUENCE = 30 
FONT_PATH = "C:/Windows/Fonts/malgun.ttf"  # ì‚¬ìš©ì í™˜ê²½ì— ë§ëŠ” í•œê¸€ í°íŠ¸ ê²½ë¡œë¡œ ì„¤ì •
CONFIDENCE_THRESHOLD = 0.8  # ì´ ê°’ ì´ìƒì˜ í™•ì‹ ë„ë¥¼ ê°€ì§ˆ ë•Œë§Œ ê²°ê³¼ë¥¼ í‘œì‹œ

# --- ëª¨ë¸ ë° ë¦¬ì†ŒìŠ¤ ë¡œë“œ ---
print("â–¶ ëª¨ë¸ê³¼ ë¦¬ì†ŒìŠ¤ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
model = load_model(MODEL_PATH)
label_encoder = joblib.load(ENCODER_PATH)
print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)
mp_drawing = mp.solutions.drawing_utils

# ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ì €ì¥í•  deque ìƒì„±
frame_buffer = deque(maxlen=FRAMES_PER_SEQUENCE)

# --- í•œê¸€ í…ìŠ¤íŠ¸ ì¶œë ¥ì„ ìœ„í•œ í•¨ìˆ˜ ---
def draw_korean_text(img, text, position, font_size=32, color=(255, 255, 255)):
    font = ImageFont.truetype(FONT_PATH, font_size)
    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    draw.text(position, text, font=font, fill=color)
    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

# --- ë©”ì¸ ë£¨í”„ ---
cap = cv2.VideoCapture(0)
print("â–¶ ì‹¤ì‹œê°„ ì„±ëŠ¥ í™•ì¸ì„ ì‹œì‘í•©ë‹ˆë‹¤. ('q'ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œ)")

prediction_result = ("", 0.0)

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    frame = cv2.flip(frame, 1)
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = hands.process(rgb_frame)

    # --- ğŸŒŸ ë°ì´í„° ìˆ˜ì§‘ê³¼ ë™ì¼í•œ ì •ê·œí™” ë¡œì§ ì ìš© ---
    hand_data = {"Left": [0.0] * 63, "Right": [0.0] * 63}
    hand_detected = {"Left": False, "Right": False}

    if results.multi_hand_landmarks:
        # 1. ëœë“œë§ˆí¬ ì ˆëŒ€ ì¢Œí‘œ ìˆ˜ì§‘
        for hand_landmarks, hand_handedness in zip(results.multi_hand_landmarks, results.multi_handedness):
            hand_label = hand_handedness.classification[0].label
            coords = [lm for lm in hand_landmarks.landmark]
            hand_data[hand_label] = coords
            hand_detected[hand_label] = True
            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

        # 2. ì†ëª© ê¸°ì¤€ ìƒëŒ€ ì¢Œí‘œë¡œ ì •ê·œí™”
        normalized_left = [0.0] * 63
        if hand_detected["Left"]:
            left_wrist = hand_data["Left"][0]
            for i, lm in enumerate(hand_data["Left"]):
                normalized_left[i*3] = lm.x - left_wrist.x
                normalized_left[i*3 + 1] = lm.y - left_wrist.y
                normalized_left[i*3 + 2] = lm.z - left_wrist.z

        normalized_right = [0.0] * 63
        if hand_detected["Right"]:
            right_wrist = hand_data["Right"][0]
            for i, lm in enumerate(hand_data["Right"]):
                normalized_right[i*3] = lm.x - right_wrist.x
                normalized_right[i*3 + 1] = lm.y - right_wrist.y
                normalized_right[i*3 + 2] = lm.z - right_wrist.z
        
        # 3. ì •ê·œí™”ëœ ì¢Œí‘œë¥¼ í•˜ë‚˜ì˜ í”„ë ˆì„ìœ¼ë¡œ í•©ì¹˜ê¸°
        one_frame = normalized_left + normalized_right
        frame_buffer.append(one_frame)
    else:
        # ì†ì´ ê°ì§€ë˜ì§€ ì•Šìœ¼ë©´ ë²„í¼ë¥¼ ë¹„ì›Œ ì˜ëª»ëœ ì˜ˆì¸¡ ë°©ì§€
        frame_buffer.clear()

    # --- ëª¨ë¸ ì˜ˆì¸¡ ---
    if len(frame_buffer) == FRAMES_PER_SEQUENCE:
        # ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ëª¨ë¸ ì…ë ¥ í˜•íƒœì— ë§ê²Œ ë³€í™˜
        sequence_data = np.array(frame_buffer).reshape(1, FRAMES_PER_SEQUENCE, 126)
        
        # ì˜ˆì¸¡ ì‹¤í–‰
        prediction = model.predict(sequence_data, verbose=0)
        confidence = np.max(prediction)
        
        if confidence >= CONFIDENCE_THRESHOLD:
            predicted_index = np.argmax(prediction)
            predicted_label = label_encoder.inverse_transform([predicted_index])[0]
            prediction_result = (predicted_label, confidence)
        else:
            prediction_result = ("?", 0.0) # í™•ì‹ ë„ê°€ ë‚®ìœ¼ë©´ '?'ë¡œ í‘œì‹œ

    # --- í™”ë©´ì— ê²°ê³¼ ì¶œë ¥ ---
    label, conf = prediction_result
    status_text = f"Prediction: {label} ({conf:.2f})"
    color = (0, 255, 0) if conf >= CONFIDENCE_THRESHOLD else (0, 0, 255) # í™•ì‹ ë„ ë†’ìœ¼ë©´ ì´ˆë¡, ë‚®ìœ¼ë©´ ë¹¨ê°•
    
    frame = draw_korean_text(frame, status_text, (10, 50), font_size=40, color=color)
    
    cv2.imshow("LSTM Model Performance Check", frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()