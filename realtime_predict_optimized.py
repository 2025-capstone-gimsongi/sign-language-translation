import cv2
import mediapipe as mp
import numpy as np
from keras.models import load_model
import joblib
from collections import deque
from PIL import ImageFont, ImageDraw, Image
import threading
import torch
from transformers import T5ForConditionalGeneration, T5TokenizerFast as T5Tokenizer

# --- ğŸ’¡ ì„¤ì •ê°’ ---
MODEL_PATH = "models/gesture_lstm_model_dual_v2.h5" 
ENCODER_PATH = "processed_lstm/label_encoder_lstm_dual.pkl"
T5_MODEL_PATH = "./my_finetuned_t5_model" # T5 ëª¨ë¸ ê²½ë¡œ
FRAMES_PER_SEQUENCE = 30 
FONT_PATH = "C:/Windows/Fonts/malgun.ttf"
CONFIDENCE_THRESHOLD = 0.75 # ë‹¨ì–´ ì¶”ê°€ë¥¼ ìœ„í•œ ìµœì†Œ í™•ì‹ ë„ (0.85 -> 0.75ë¡œ í•˜í–¥ ì¡°ì •)
PREDICTION_INTERVAL = 3 # ì˜ˆì¸¡ ê°„ê²© (5í”„ë ˆì„ë§ˆë‹¤ í•œ ë²ˆ -> 3í”„ë ˆì„ë§ˆë‹¤ í•œ ë²ˆ)

# --- ğŸ’¡ ì „ì—­ ë³€ìˆ˜ ---
prediction_result = ("", 0.0)
sentence_words = []
generated_sentence = ""
is_predicting = False

# --- ëª¨ë¸ ë° ë¦¬ì†ŒìŠ¤ ë¡œë“œ ---
print("â–¶ ëª¨ë¸ê³¼ ë¦¬ì†ŒìŠ¤ ë¡œë“œ ì¤‘...")
model = load_model(MODEL_PATH)
label_encoder = joblib.load(ENCODER_PATH)
tokenizer = T5Tokenizer.from_pretrained(T5_MODEL_PATH)
t5_model = T5ForConditionalGeneration.from_pretrained(T5_MODEL_PATH)
print("âœ… ëª¨ë“  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)
mp_drawing = mp.solutions.drawing_utils

frame_buffer = deque(maxlen=FRAMES_PER_SEQUENCE)

# --- í•œê¸€ í…ìŠ¤íŠ¸ ì¶œë ¥ í•¨ìˆ˜ (ìë™ ì¤„ ë°”ê¿ˆ ê¸°ëŠ¥ ì¶”ê°€) ---
def draw_korean_text(img, text, position, font_size=32, color=(255, 255, 255), max_width=None):
    font = ImageFont.truetype(FONT_PATH, font_size)
    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)

    x, y = position
    line_height = font_size + 5 # ì¤„ ê°„ê²© ì¡°ì ˆ

    # ìµœëŒ€ ë„ˆë¹„ê°€ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ ì´ë¯¸ì§€ ë„ˆë¹„ì˜ 90%ë¥¼ ì‚¬ìš© (ê¸°ë³¸ê°’)
    if max_width is None:
        max_width = img.shape[1] * 0.9 - x

    words = text.split(' ')
    current_line = []
    current_y = y

    for word in words:
        test_line = ' '.join(current_line + [word])
        # í…ìŠ¤íŠ¸ ë„ˆë¹„ ì¸¡ì • (PIL.ImageDraw.Draw.textsize ì‚¬ìš©)
        bbox = font.getbbox(test_line)
        text_width = bbox[2] - bbox[0]  # ë„ˆë¹„ = right - left

        if text_width < max_width:
            current_line.append(word)
        else:
            # í˜„ì¬ ì¤„ ì¶œë ¥
            draw.text((x, current_y), ' '.join(current_line), font=font, fill=color)
            current_y += line_height # ë‹¤ìŒ ì¤„ë¡œ ì´ë™
            current_line = [word] # ìƒˆ ì¤„ ì‹œì‘

    # ë§ˆì§€ë§‰ ì¤„ ì¶œë ¥
    if current_line:
        draw.text((x, current_y), ' '.join(current_line), font=font, fill=color)

    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

# --- T5 ë¬¸ì¥ ìƒì„± ìŠ¤ë ˆë“œ í•¨ìˆ˜ ---
def generate_sentence_with_t5(words):
    global generated_sentence
    if not words:
        return
    
    prompt = f"ë¬¸ì¥ ìƒì„±: {', '.join(words)}"
    print(f"ğŸ“ T5 ì…ë ¥: '{prompt}'")
    
    inputs = tokenizer(prompt, return_tensors="pt")
    with torch.no_grad():
        outputs = t5_model.generate(
            inputs.input_ids, max_length=64, num_beams=5, early_stopping=True,
            repetition_penalty=2.0, no_repeat_ngram_size=2
        )
    result_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)
    generated_sentence = result_sentence
    print(f"âœ… T5 ìƒì„± ë¬¸ì¥: {generated_sentence}")

# --- LSTM ì œìŠ¤ì²˜ ì˜ˆì¸¡ ìŠ¤ë ˆë“œ í•¨ìˆ˜ ---
def predict_gesture(sequence_data):
    global prediction_result, is_predicting, sentence_words, generated_sentence
    
    prediction = model.predict(sequence_data, verbose=0)
    confidence = np.max(prediction)
    predicted_index = np.argmax(prediction)
    predicted_label = label_encoder.inverse_transform([predicted_index])[0]
    
    prediction_result = (predicted_label, confidence)
    
    # í™•ì‹ ë„ê°€ ì¶©ë¶„íˆ ë†’ê³ , ë§ˆì§€ë§‰ ë‹¨ì–´ì™€ ì¤‘ë³µë˜ì§€ ì•Šì„ ë•Œë§Œ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    if confidence >= CONFIDENCE_THRESHOLD and (not sentence_words or sentence_words[-1] != predicted_label):
        if predicted_label == "OK": # 'OK' ì œìŠ¤ì²˜ê°€ ì¸ì‹ë˜ë©´ ë¬¸ì¥ ìƒì„± ì‹œì‘
            if sentence_words: # ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ ìƒì„±
                threading.Thread(target=generate_sentence_with_t5, args=(list(sentence_words),), daemon=True).start()
                sentence_words.clear() # ë¬¸ì¥ ìƒì„± í›„ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        else:
            generated_sentence = "" # ìƒˆë¡œìš´ ë‹¨ì–´ê°€ ì¶”ê°€ë˜ë©´ ì´ì „ ë¬¸ì¥ ê²°ê³¼ëŠ” ì§€ì›€
            sentence_words.append(predicted_label)
            print(f"â• ë‹¨ì–´ ì¶”ê°€: {predicted_label} (í˜„ì¬ ë¦¬ìŠ¤íŠ¸: {sentence_words})")
            
    is_predicting = False

# --- ë©”ì¸ ë£¨í”„ ---
cap = cv2.VideoCapture(0)
if not cap.isOpened():
    print("âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."); exit()

print("â–¶ ì‹¤ì‹œê°„ ìˆ˜ì–´ ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤. ('q'ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œ)")
frame_count = 0

while cap.isOpened():
    ret, frame = cap.read()
    if not ret: break

    frame = cv2.flip(frame, 1)
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = hands.process(rgb_frame)

    # ë°ì´í„° ì •ê·œí™” ë° ë²„í¼ ì¶”ê°€
    if results.multi_hand_landmarks and results.multi_handedness:
        hand_data = {"Left": [], "Right": []}
        for hand_landmarks, hand_handedness in zip(results.multi_hand_landmarks, results.multi_handedness):
            hand_label = hand_handedness.classification[0].label
            hand_data[hand_label] = [lm for lm in hand_landmarks.landmark]

        normalized_left = [0.0] * 63
        if hand_data["Left"]:
            left_wrist = hand_data["Left"][0]
            for i, lm in enumerate(hand_data["Left"]):
                normalized_left[i*3:i*3+3] = [lm.x - left_wrist.x, lm.y - left_wrist.y, lm.z - left_wrist.z]

        normalized_right = [0.0] * 63
        if hand_data["Right"]:
            right_wrist = hand_data["Right"][0]
            for i, lm in enumerate(hand_data["Right"]):
                normalized_right[i*3:i*3+3] = [lm.x - right_wrist.x, lm.y - right_wrist.y, lm.z - right_wrist.z]
        
        one_frame = normalized_left + normalized_right
        frame_buffer.append(one_frame)
    else:
        frame_buffer.clear()

    # ì˜ˆì¸¡ ìŠ¤ë ˆë“œ ì‹¤í–‰
    frame_count += 1
    if len(frame_buffer) == FRAMES_PER_SEQUENCE and not is_predicting and frame_count % PREDICTION_INTERVAL == 0:
        is_predicting = True
        sequence_data = np.array(frame_buffer).reshape(1, FRAMES_PER_SEQUENCE, 126)
        threading.Thread(target=predict_gesture, args=(sequence_data,), daemon=True).start()

    # --- í™”ë©´ ì¶œë ¥ ---
    if results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)
            
    # 1. í˜„ì¬ ëª¨ë¸ì˜ ìµœê³  ì¶”ì¸¡ ë‹¨ì–´ í‘œì‹œ (ì‹¤ì‹œê°„ í”¼ë“œë°±ìš©)
    label, conf = prediction_result
    feedback_text = f"Guess: {label} ({conf:.2f})"
    color = (0, 255, 0) if conf >= CONFIDENCE_THRESHOLD else (255, 0, 0)
    frame = draw_korean_text(frame, feedback_text, (10, 30), font_size=32, color=color)

    # 2. í˜„ì¬ê¹Œì§€ ì…ë ¥ëœ ë‹¨ì–´ ëª©ë¡ í‘œì‹œ
    words_text = " ".join(sentence_words)
    frame = draw_korean_text(frame, words_text, (10, 80), font_size=40, color=(0, 0, 0), max_width=frame.shape[1] - 20)
    
    # 3. T5ê°€ ìƒì„±í•œ ìµœì¢… ë¬¸ì¥ í‘œì‹œ (ì¤„ ë°”ê¿ˆ ì ìš©)
    if generated_sentence:
        frame = draw_korean_text(frame, generated_sentence, (10, 130), font_size=40, color=(0, 0, 0), max_width=frame.shape[1] - 20)

    cv2.imshow("Sign Language Translator (Optimized + T5)", frame)
    # 1. waitKeyë¥¼ í•œ ë²ˆë§Œ í˜¸ì¶œí•´ì„œ ê·¸ ê²°ê³¼ë¥¼ key ë³€ìˆ˜ì— ì €ì¥í•©ë‹ˆë‹¤.
    key = cv2.waitKey(1) & 0xFF

    # 2. ì €ì¥ëœ key ë³€ìˆ˜ì˜ ê°’ì„ ë¹„êµí•©ë‹ˆë‹¤.
    # 'q'ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œ
    if key == ord('q'):
        break
    # ìŠ¤í˜ì´ìŠ¤ë°”ë¥¼ ëˆ„ë¥´ë©´ ì´ˆê¸°í™”
    elif key == ord(' '):
        sentence_words.clear()
        generated_sentence = ""
        prediction_result = ("", 0.0)
        print("ğŸ”„ ë¬¸ì¥ ë° ë‹¨ì–´ ëª©ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

cap.release()
cv2.destroyAllWindows()