# lstm ëª¨ë¸ í•™ìŠµ ì½”ë“œ

import numpy as np
import os
import datetime
from keras.models import Sequential, load_model # ğŸŒŸ load_model ì¶”ê°€
from keras.layers import LSTM, Dense, Dropout
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard
from sklearn.model_selection import StratifiedShuffleSplit

# --- âš™ï¸ ì„¤ì •: ì´ ìŠ¤ìœ„ì¹˜ë¡œ ëª¨ë“œë¥¼ ë³€ê²½í•˜ì„¸ìš” ---
# True: model_save_pathì— ìˆëŠ” ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ ì´ì–´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.
# False: ê¸°ì¡´ì²˜ëŸ¼ ìƒˆë¡œìš´ ëª¨ë¸ì„ ì²˜ìŒë¶€í„° í•™ìŠµí•©ë‹ˆë‹¤.
CONTINUE_TRAINING = False
# -----------------------------------------

def train_lstm_model_dual(X_path, y_path, model_save_path):
    X = np.load(X_path)
    y = np.load(y_path)

    print(f"ğŸ”¹ X shape: {X.shape}")
    print(f"ğŸ”¹ y shape: {y.shape}")

    y_classes = np.argmax(y, axis=1)

    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
    train_idx, val_idx = next(sss.split(X, y_classes))
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]

    # --- ğŸ‘‡ ì—¬ê¸°ê°€ í•µì‹¬ ë³€ê²½ ë¶€ë¶„ì…ë‹ˆë‹¤ ğŸ‘‡ ---
    if CONTINUE_TRAINING and os.path.exists(model_save_path):
        print(f"ğŸ“– ê¸°ì¡´ ëª¨ë¸ '{model_save_path}'ì„(ë¥¼) ë¶ˆëŸ¬ì™€ ì´ì–´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.")
        model = load_model(model_save_path)
    else:
        print("âœ¨ ìƒˆë¡œìš´ ëª¨ë¸ì„ ì²˜ìŒë¶€í„° í•™ìŠµí•©ë‹ˆë‹¤.")
        model = Sequential([
            LSTM(128, return_sequences=True, input_shape=(X.shape[1], X.shape[2])),
            LSTM(64, return_sequences=False),
            Dense(64, activation='relu'),
            Dropout(0.5),
            Dense(y.shape[1], activation='softmax')
        ])
    # --- ğŸ‘† ì—¬ê¸°ê¹Œì§€ê°€ í•µì‹¬ ë³€ê²½ ë¶€ë¶„ì…ë‹ˆë‹¤ ğŸ‘† ---

    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    model.summary()

    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
    
    checkpoint = ModelCheckpoint(model_save_path, monitor='val_loss', save_best_only=True, verbose=1, mode='min')
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, mode='min')
    log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

    print("â–¶ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    model.fit(
        X_train, y_train,
        epochs=100,
        batch_size=16,
        validation_data=(X_val, y_val),
        callbacks=[checkpoint, early_stopping, reduce_lr, tensorboard_callback]
    )

    print(f"âœ… ì–‘ì† LSTM ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ: {model_save_path}")

if __name__ == "__main__":
    train_lstm_model_dual(
        X_path="processed_lstm/X_seq_lstm_dual.npy",
        y_path="processed_lstm/y_seq_lstm_dual.npy",
        # ğŸŒŸ ë¶ˆëŸ¬ì˜¤ê³  ì €ì¥í•  ëª¨ë¸ íŒŒì¼ ê²½ë¡œë¥¼ ì •í™•íˆ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.
        model_save_path="models/gesture_lstm_model_dual_v2.h5" 
    )